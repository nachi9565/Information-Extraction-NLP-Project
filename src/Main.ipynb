{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import errno\n",
    "import spacy\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import neuralcoref\n",
    "import copy\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from itertools import chain\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import Tree\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.corpus import stopwords\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Doc\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ps = PorterStemmer()\n",
    "check_list=[\"part\",\"belong\",\"in\",\"citi\",\"insid\",\"capit\",\"state\",\"orgin\",\"is\",\"locat\"]\n",
    "job_list=['executive', 'actress', 'host', 'producer', 'philanthropist', 'queen', 'barber', 'president', 'miner', 'city councilman', 'farmer', 'preacher', 'maid', 'student', 'news anchor', 'critic', 'columnist', 'candidate', 'author', 'housewife', 'judge', 'princess', 'personal trainer', 'reader', 'model student', 'journalist', 'biographer', 'reporter', 'king', 'filmmaker', 'editor', 'therapist', 'entertainer', 'ceo', 'senator', 'chairman', 'politician', 'leader', 'pope', 'springer', 'professor', 'attorney', 'governor', 'crown prince', 'teacher', 'premier', 'mayor', 'magician', 'executive producer', 'magnate', 'vice president', 'founder', 'congressman', 'stockbroker', 'salesman', 'analyst', 'general', 'janitor', 'boss', 'doctor', 'activist', 'owner', 'director', 'trader', 'chief financial officer', 'publisher', 'companion', 'assistant coach', 'manager', 'mediator', \n",
    "          'secretary of the treasury', 'actuary', 'manufacturer', 'river', 'surveyor', 'lieutenant governor', 'commander', 'envoy', 'lieutenant colonel', 'translator', 'captain', 'colonel', 'commander colonel', 'brigadier general', 'major general', 'guard', 'soldier', 'secretary', 'baron', 'chief of staff', 'major', 'admiral', 'president general', 'coach', 'chancellor', 'administrator', 'merchant', 'attorney general', 'secretary of state', 'secretary of war', 'diplomat', 'chief justice', 'negotiator', 'minister', 'principal', 'secretary of treasury', 'historian', 'lieutenant general', 'speaker', 'reverend', 'architect', 'dentist', 'dancer', 'pastor', 'creator', 'charter', 'entrepreneur', 'engineer', 'designer', 'co-founder', 'co-chairman', 'model', 'pilot', 'sailor', 'commodore', 'guide', 'chief executive officer', 'chief technology officer', 'astronaut', 'scientist', \n",
    "          'gen.', 'geographer', 'emperor', 'theologian', 'printer manufacturer', 'recorder', 'general manager', 'salesmen', 'vendor', 'graphic designer', 'inventor', 'secretary of housing and urban development', 'secretary of transportation', 'referee', 'dealer', 'driver', 'collector', 'vice-president', 'demonstrator', 'cell maker', 'private', 'spokesman', 'buyer', 'cfo', 'managing director', 'chief executive', 'retailer', 'printer', 'developer', 'processor', 'grip', 'chief operating officer', 'assistant', 'layer', 'operator', 'header', 'writer', 'singer', 'evangelist', 'executive director', 'general counsel', 'city manager', 'physician', 'importer', 'explorer', 'empress', 'boxer', 'general secretary', 'party leader', 'representative', 'secretary of defense', 'prince', 'director-general', 'fund manager', 'surgeon', 'cook', 'comptroller', 'refiner', 'tanker', 'vice-chairman', \n",
    "          'executive chairman', 'constable', 'interim president', 'nobel laureate', 'dean', 'artist', 'landscape architect', 'consultant', 'chef', 'vice chairman', 'superior', 'jeweler', 'specialist', 'broker', 'strategist', 'treasury secretary', 'underwriter', 'quality control supervisor', 'auditor', 'spokeswoman', 'district attorney', 'principal author', 'treasurer', 'lobbyist', 'deputy mayor', 'communications director', 'assistant attorney general', 'executive vice president', 'chief compliance officer', 'lawyer', 'spokesperson', 'technician', 'intelligence director', 'hacker', 'astronomer', 'composer', 'aerospace engineer', 'homemaker', 'marketing manager', 'businesswoman', 'monk', 'explorer captain', 'builder', 'state treasurer', 'superintendent', 'governor general', 'prime minister', 'chief minister', 'poet', 'novelist', 'indian activist', 'clerk', 'barrister', 'priest', \n",
    "          'landlady', 'magistrate', 'police officer', 'saint', 'dictator', 'representative leader', 'governor-general', 'marshal', 'philosopher', 'butcher', 'missionary', 'sultan', 'interpreter', 'economist', 'physicist', 'musician', 'custodian', 'investment banker', 'financier', 'secretary of commerce', 'secretary of labor', 'performer', 'legislator', 'actor', 'cabinetmaker', 'carpenter', 'servant', 'ambassador', 'chief of staff general', 'rep.', 'campaign manager', 'jurist', 'whig activist', 'orderly', 'sociologist', 'bishop', 'botanist', 'sheriff', 'chief of police', 'firefighter', 'cartographer', 'lt. col.', 'anthropologist', 'minority leader', 'food critic', 'playwright', 'cowboy', 'first lady', 'agriculture commissioner', 'corporal', 'flyer', 'software engineer', 'navigator', 'businessman', 'steward', 'comedian', 'grocer', 'student activist', 'machinist', 'hatter', 'babysitter', \n",
    "          'waitress', 'computer scientist', 'tipper', 'hockey player', 'researcher', 'broadcaster', 'thinner']\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = [{\"label\": \"BUY\", \"pattern\": \"purchased\"}, {\"label\": \"BUY\", \"pattern\": \"purchased by\"},\n",
    "                {\"label\": \"BUY\", \"pattern\": \"acquired by\"}, {\"label\": \"BUY\", \"pattern\": \"acquired\"},\n",
    "            {\"label\": \"BUY\", \"pattern\": \"acquire\"}, {\"label\": \"BUY\", \"pattern\": \"bought\"}, \n",
    "            {\"label\": \"BUY\", \"pattern\": \"bought  by\"},{\"label\": \"BUY\", \"pattern\": \"took over\"},\n",
    "           {\"label\": \"BUY\", \"pattern\": \"owns\"},{\"label\": \"BUY\", \"pattern\": \"owned\"},\n",
    "            {\"label\": \"BUY\", \"pattern\": \"own\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def read_single_file_with_coref(filename):\n",
    "    sent_tokens = []\n",
    "    f = open(filename,encoding=\"ascii\",errors=\"ignore\")\n",
    "    temp=f.read()\n",
    "    temp=nlp(temp)\n",
    "    temp=temp._.coref_resolved\n",
    "    for sent in nltk.sent_tokenize(temp):\n",
    "        sent_tokens.extend(sent.split('\\n\\n'))\n",
    "    f.close()\n",
    "    return sent_tokens\n",
    "\n",
    "def read_single_file_table(file_name):\n",
    "    f = open(file_name, encoding=\"ascii\", errors=\"ignore\")\n",
    "    lines = f.readlines()\n",
    "    sentences = ''\n",
    "    for line in lines:\n",
    "        if line.find('    ') == 0 or '\\t' in line:\n",
    "            sentences = sentences + line.replace('\\n','.')\n",
    "    f.close()\n",
    "    sent_tokens =  sent_tokenize(sentences)\n",
    "    return sent_tokens\n",
    "\n",
    "def merge_ents(doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(ent)\n",
    "    return doc\n",
    "\n",
    "def check_job(text):\n",
    "    text=text.split()\n",
    "    for i in text:\n",
    "        if(i.lower() in job_list):\n",
    "            return True\n",
    "    return False\n",
    "#///////////////////////////////////////////////BUY TEMPLATE\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "\n",
    "def extract_buy_templates(doc):\n",
    "    template = {\"buyer\": \"\", \"item\": \"\", \"price\": \"\", \"quantity\": \"\", \"source\": \"\"}\n",
    "    list_of_templates = []\n",
    "    \n",
    "    # A BUY B for MONEY\n",
    "    for head in doc:\n",
    "        if(head.ent_type_==\"BUY\"):\n",
    "            for token in head.children:\n",
    "                if (token.dep_ == \"nsubj\"):\n",
    "                    template[\"buyer\"] = token\n",
    "                if((token.pos_==\"NOUN\" and token.dep_ == \"dobj\")):\n",
    "                    template[\"item\"] = token\n",
    "                    for j in token.children:\n",
    "                        if(j.dep_==\"nummod\"):\n",
    "                            template[\"quantity\"]=j\n",
    "                elif (token.dep_ == \"dobj\"):\n",
    "                    template[\"item\"] = token\n",
    "                for i in doc:\n",
    "                    if(i.ent_type_ == \"MONEY\"):\n",
    "                        if(head in list(i.ancestors)):\n",
    "                            template[\"price\"] = i\n",
    "                    if(i.text.lower()==\"from\" or i.text.lower()==\"of\" or i.text.lower()==\"in\"):\n",
    "                        for j in i.children:\n",
    "                            if(j.pos_==\"PROPN\" and j.ent_type_!=\"GPE\"):\n",
    "                                template[\"source\"]=j\n",
    "                if (len(template[\"buyer\"]) > 0 and len(template[\"item\"]) > 0):\n",
    "                    list_of_templates.append(template)\n",
    "                    template = {\"buyer\": \"\", \"item\": \"\", \"price\": \"\", \"quantity\": \"\", \"source\": \"\"}\n",
    "    \n",
    "    # B was BUY by A for MONEY\n",
    "    \n",
    "    for head in doc:\n",
    "        if(head.ent_type_==\"BUY\"):\n",
    "            for token in head.children:\n",
    "                if((token.pos_==\"NOUN\" and token.dep_ == \"nsubjpass\")):\n",
    "                    template[\"item\"] = token\n",
    "                    for j in token.children:\n",
    "                        if(j.dep_==\"nummod\"):\n",
    "                            template[\"quantity\"]=j\n",
    "                elif ( token.dep_ == \"nsubjpass\"):\n",
    "                    template[\"item\"] = token\n",
    "                if ( token.dep_ == \"pobj\" ):\n",
    "                    template[\"buyer\"] = token\n",
    "                \n",
    "                for i in doc:\n",
    "                    if(i.ent_type_ == \"MONEY\"):\n",
    "                        if(head in list(i.ancestors)):\n",
    "                            template[\"price\"] = i\n",
    "                    if(i.text.lower()==\"from\" or i.text.lower()==\"of\" or i.text.lower()==\"in\"):\n",
    "                        for j in i.children:\n",
    "                            if(j.pos_==\"PROPN\" and j.ent_type_!=\"GPE\"):\n",
    "                                template[\"source\"]=j\n",
    "                if (len(template[\"buyer\"]) > 0 and len(template[\"item\"]) > 0):\n",
    "                    list_of_templates.append(template)\n",
    "                    template = {\"buyer\": \"\", \"item\": \"\", \"price\": \"\", \"quantity\": \"\", \"source\": \"\"}\n",
    "    \n",
    "    return list_of_templates\n",
    "\n",
    "#///////////////////////////////////////////////PART TEMPLATE\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "\n",
    "# PARSE TREE APPROCH \n",
    "\n",
    "def place_parse_tree(text):\n",
    "    doc = nlp(text)\n",
    "    doc=merge_ents(doc)\n",
    "    \n",
    "    def check(text):\n",
    "        count=0\n",
    "        for token in doc:\n",
    "            if(token.text==text):\n",
    "                temp=list(token.ancestors)\n",
    "                for i in temp:\n",
    "                    if(ps.stem(i.text) in check_list):\n",
    "                         count+=1\n",
    "                break\n",
    "        if(count>1):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    left_list=[]\n",
    "    temp_str=\"\"\n",
    "    for sent in doc.sents:\n",
    "        for child in sent.root.children:\n",
    "            if(child.ent_type_ == \"GPE\"):\n",
    "                left_list.append(child.text)\n",
    "                break\n",
    "    if(len(left_list)==0):\n",
    "        return []\n",
    "\n",
    "    next_ind=text.find(left_list[0])+len(left_list[0])+2\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ == \"GPE\" and ent.text not in left_list):\n",
    "            if(next_ind==ent.start_char):\n",
    "                left_list.append(ent.text)\n",
    "                next_ind=ent.end_char+2\n",
    "            elif(next_ind>0 and next_ind+3<len(text) and (next_ind+3==ent.start_char or next_ind+4==ent.start_char) and (text[next_ind-1:next_ind+2]==\"and\" or text[next_ind:next_ind+3]==\"and\")):\n",
    "                left_list.append(\"&\")\n",
    "                left_list.append(ent.text)\n",
    "                next_ind=ent.end_char+2\n",
    "\n",
    "\n",
    "    right_list=[]\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ == \"GPE\" and ent.text not in left_list and check(ent.text)==True):\n",
    "            right_list.append(ent.text)\n",
    "            break\n",
    "\n",
    "    if(len(right_list)==0):\n",
    "        return []\n",
    "    next_ind=text.find(right_list[0])+len(right_list[0])+2\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ == \"GPE\" and ent.text not in left_list and ent.text not in right_list and check(ent.text)==True):\n",
    "            if(next_ind==ent.start_char):\n",
    "                right_list.append(ent.text)\n",
    "                next_ind=ent.end_char+2\n",
    "            elif(next_ind>0 and next_ind+3<len(text) and (next_ind+3==ent.start_char or next_ind+4==ent.start_char) and (text[next_ind-1:next_ind+2]==\"and\" or text[next_ind:next_ind+3]==\"and\")):\n",
    "                right_list.append(\"&\")\n",
    "                right_list.append(ent.text)\n",
    "                next_ind=ent.end_char+2;\n",
    "\n",
    "    parse_ans=[]\n",
    "    if(len(left_list)==1 and len(right_list)==1):\n",
    "        temp=(left_list[0],right_list[0])\n",
    "        parse_ans.append(temp)\n",
    "    elif(\"&\" in left_list and \"&\" in right_list):\n",
    "        left_list.remove(\"&\")\n",
    "        right_list.remove(\"&\")\n",
    "        for i in left_list:\n",
    "            for j in right_list:\n",
    "                temp=(i,j)\n",
    "                parse_ans.append(temp)\n",
    "    elif(\"&\" in left_list and \"&\" not in right_list):\n",
    "        left_list.remove(\"&\")\n",
    "        for i in left_list:\n",
    "            temp=(i,right_list[0])\n",
    "            parse_ans.append(temp)\n",
    "    return parse_ans\n",
    "\n",
    "# Holonyms Approch\n",
    "\n",
    "def place_holonym(text):\n",
    "    doc = nlp(text)\n",
    "    doc=merge_ents(doc)\n",
    "    def holo(word):\n",
    "        holonyms_list = []\n",
    "        for i,j in enumerate(wn.synsets(word)):\n",
    "            holonyms_list.extend(list(chain(*[l.lemma_names() for l in j.part_holonyms()])))\n",
    "        return holonyms_list\n",
    "    holo_ans=[]\n",
    "    gpe_ents=[]\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ == \"GPE\"):\n",
    "            gpe_ents.append(ent.text)\n",
    "    for i in gpe_ents:\n",
    "        ans_list=holo(i.replace(\" \",\"_\"))\n",
    "        for j in gpe_ents:\n",
    "            if (j.replace(\" \",\"_\") in ans_list):\n",
    "                anss=(i,j)\n",
    "                holo_ans.append(anss)\n",
    "    final_holo=[]\n",
    "    for i in holo_ans:\n",
    "        if(i[0]!=i[1]):\n",
    "            final_holo.append(i)\n",
    "    return final_holo\n",
    "\n",
    "#  REGEX APPROCH\n",
    "\n",
    "def place_regex(text):\n",
    "    doc = nlp(text)\n",
    "    doc=merge_ents(doc)\n",
    "    ans=[]\n",
    "    next_ind=-1\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ == \"GPE\"):\n",
    "            if(next_ind==-1 or next_ind==ent.start_char):\n",
    "                ans.append(ent.text)\n",
    "                next_ind=ent.end_char+2\n",
    "            elif(next_ind>0 and next_ind+3<len(text) and (next_ind+3==ent.start_char or next_ind+4==ent.start_char) and (text[next_ind-1:next_ind+2]==\"and\" or text[next_ind:next_ind+3]==\"and\")):\n",
    "                ans.append(\"&\")\n",
    "                ans.append(ent.text)\n",
    "                next_ind=ent.end_char+2;\n",
    "            else:\n",
    "                ans.append(\"#\")\n",
    "                ans.append(ent.text)\n",
    "                next_ind=ent.end_char+2;\n",
    "    rem_list=[]\n",
    "    for i in range(len(ans)):\n",
    "        if(ans[i]==\"&\"):\n",
    "            rem_list.append(i)\n",
    "            rem_list.append(i+1)\n",
    "            j=i-1\n",
    "            while(j!=0 and ans[j+1]!=\"#\"):\n",
    "                rem_list.append(j)\n",
    "                j=j-1\n",
    "    rem_list = list(dict.fromkeys(rem_list))\n",
    "    for index in sorted(rem_list, reverse=True):\n",
    "        del ans[index]\n",
    "    ans_regex=[]\n",
    "    temp_tup=()\n",
    "    for i in (range(len(ans)-1)):\n",
    "        if(ans[i+1]==\"#\" or ans[i]==\"#\"):\n",
    "            continue\n",
    "        else:\n",
    "            temp_tup=(ans[i],ans[i+1])\n",
    "            ans_regex.append(temp_tup)\n",
    "    return ans_regex\n",
    "\n",
    "# MERGING \n",
    "\n",
    "def place_template(text):\n",
    "    final_ans=set()\n",
    "    final_ans.update(place_regex(text))\n",
    "    final_ans.update(place_holonym(text))\n",
    "    final_ans.update(place_parse_tree(text))\n",
    "    final_ans = list(final_ans)\n",
    "    #print(\"REGEX\",place_regex(text))\n",
    "    #print(\"HOLO\",place_holonym(text))\n",
    "    #print(\"PARSE\",place_parse_tree(text))\n",
    "    return final_ans\n",
    "\n",
    "#///////////////////////////////////////////////WORK TEMPLATE\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "def extract_person_pos_relations(doc):\n",
    "    relations = []\n",
    "    work_list = []\n",
    "    for person in filter(lambda w: w.ent_type_ == 'PERSON', doc):\n",
    "        work = ()\n",
    "        pos_final = []\n",
    "        org_final = []\n",
    "        gpe_final = []\n",
    "        if person.dep_ in ('nsubj'):\n",
    "            position = [w for w in person.head.rights if w.dep_ == 'attr']\n",
    "            if position:\n",
    "                position = position[0]\n",
    "                relations.append((person, position))\n",
    "                pos_final.append(position)\n",
    "                def extract_conjuncts(position):\n",
    "                    for curr_pos in position.conjuncts:\n",
    "                        relations.append((person, curr_pos))\n",
    "                        pos_final.append(curr_pos)\n",
    "                extract_conjuncts(position)\n",
    "        elif person.dep_ == 'nsubj' and person.head.dep_ == 'root':\n",
    "            position = [w for w in person.head.lefts if w.dep_ == 'nsubj']\n",
    "            relations.append((person, position))\n",
    "            pos_final.append(position)\n",
    "        elif person.dep_ == 'ROOT':\n",
    "            if len(list(doc.sents)) > 1:\n",
    "                root = list(doc.sents)[1].root\n",
    "                position = [w for w in root.rights if w.dep_ == 'attr']\n",
    "                if position:\n",
    "                    position = position[0]\n",
    "                    relations.append((person, position))\n",
    "                    pos_final.append(position)\n",
    "                    def extract_conjuncts(position):\n",
    "                        for curr_pos in position.conjuncts:\n",
    "                            relations.append((person, curr_pos))\n",
    "                            pos_final.append(curr_pos)\n",
    "                    extract_conjuncts(position)\n",
    "        for who in filter(lambda w: w.text.lower() == 'who'.lower(), doc):\n",
    "            if who.dep_ == 'nsubj':\n",
    "                who_prep = [w for w in who.head.rights if w.dep_ == 'prep' and w.text == 'as']\n",
    "                if who_prep:\n",
    "                    who_prep = who_prep[0]\n",
    "                    position = [w for w in who_prep.rights if w.dep_ == 'pobj']\n",
    "                    if position:\n",
    "                        position = position[0]\n",
    "                        relations.append((person, position))\n",
    "                        pos_final.append(position)\n",
    "        for pos in pos_final:\n",
    "            pos_prep = [w for w in pos.rights if w.dep_ == 'prep']\n",
    "            if pos_prep:\n",
    "                pos_prep = pos_prep[0]\n",
    "                org = [w for w in pos_prep.rights if w.ent_type_ == 'ORG']\n",
    "                if org:\n",
    "                    org = org[0]\n",
    "                    org_final.append(org)\n",
    "                if not org:\n",
    "                    gpe = [w for w in pos_prep.rights if w.ent_type_ == 'GPE']\n",
    "                    if gpe:\n",
    "                        gpe = gpe[0]\n",
    "                        gpe_final.append(gpe)\n",
    "        for org in filter(lambda w: w.ent_type_ == 'GPE', doc):\n",
    "            relations.append((person, org))\n",
    "            org_final.append(org)\n",
    "        if len(list(set(gpe_final))) == 1:\n",
    "            gpe_final = gpe_final[0]\n",
    "        work = (person, list(set(pos_final)), list(set(org_final)), gpe_final)\n",
    "        work_list.append(work)\n",
    "    return relations, work_list\n",
    "\n",
    "def extract_person_pos_relations_table(doc):\n",
    "    relations = []\n",
    "    work_list = []\n",
    "    for person in filter(lambda w: w.ent_type_ == 'PERSON', doc):\n",
    "        work = ()\n",
    "        pos_final = []\n",
    "        org_final = []\n",
    "        gpe_final = []\n",
    "        if person.dep_ == 'ROOT':\n",
    "            position = [w for w in person.rights if w.dep_ == 'attr' or w.dep_ == 'appos']\n",
    "            if position:\n",
    "                position = position[0]\n",
    "                if position.ent_type_ == '':\n",
    "                    relations.append((person, position))\n",
    "                    pos_final.append(position)\n",
    "                def extract_conjuncts(position):\n",
    "                    for curr_pos in position.conjuncts:\n",
    "                        if curr_pos.ent_type_ == '':\n",
    "                            relations.append((person, curr_pos))\n",
    "                            pos_final.append(curr_pos)\n",
    "                extract_conjuncts(position)\n",
    "        for org in filter(lambda w: w.ent_type_ == 'GPE', doc):\n",
    "            relations.append((person, org))\n",
    "            org_final.append(org)\n",
    "        if len(list(set(gpe_final))) == 1:\n",
    "            gpe_final = gpe_final[0]\n",
    "        work = (person, list(set(pos_final)), list(set(org_final)), gpe_final)\n",
    "        work_list.append(work)\n",
    "    return relations, work_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON File for 1.txt  printed to current working directory\n",
      "JSON File for 2.txt  printed to current working directory\n",
      "JSON File for 3.txt  printed to current working directory\n",
      "JSON File for 4.txt  printed to current working directory\n",
      "JSON File for 5.txt  printed to current working directory\n",
      "Extraction Done\n"
     ]
    }
   ],
   "source": [
    "myFiles = glob.glob('*.txt')\n",
    "for filename in myFiles:\n",
    "    test_sentences=read_single_file_with_coref(filename)\n",
    "    final_dict={}\n",
    "    final_dict[\"document\"]=filename\n",
    "    final_dict[\"extraction\"]=[]\n",
    "    for sentence_text in test_sentences:\n",
    "        try:\n",
    "            sentence = nlp(sentence_text)\n",
    "            sentence=merge_ents(sentence)\n",
    "            #PART\n",
    "            temp=place_template(sentence_text)\n",
    "            if(temp!=[]):\n",
    "                for j in temp:\n",
    "                    temp_dict={}\n",
    "                    temp_dict[\"template\"]=\"PART\"\n",
    "                    temp_dict[\"sentences\"]=[]\n",
    "                    temp_dict[\"sentences\"].append(sentence_text)\n",
    "                    temp_dict[\"arguments\"]={}\n",
    "                    temp_dict[\"arguments\"][\"1\"]=j[0]\n",
    "                    temp_dict[\"arguments\"][\"2\"]=j[1]\n",
    "                    final_dict[\"extraction\"].append(temp_dict)\n",
    "            #BUY\n",
    "            ans=extract_buy_templates(sentence)\n",
    "            if(ans!=[]):\n",
    "                for i in ans:\n",
    "                    temp_dict={}\n",
    "                    temp_dict[\"template\"]=\"BUY\"\n",
    "                    temp_dict[\"sentences\"]=[]\n",
    "                    temp_dict[\"sentences\"].append(sentence.text)\n",
    "                    temp_dict[\"arguments\"]={}\n",
    "                    temp_dict[\"arguments\"][\"1\"]=i[\"buyer\"].text\n",
    "                    temp_dict[\"arguments\"][\"2\"]=i[\"item\"].text\n",
    "                    if(len(i[\"price\"])==0):\n",
    "                        temp_dict[\"arguments\"][\"3\"]=i[\"price\"]\n",
    "                    else:\n",
    "                        temp_dict[\"arguments\"][\"3\"]=i[\"price\"].text\n",
    "                    if(len(i[\"quantity\"])==0):\n",
    "                        temp_dict[\"arguments\"][\"4\"]=i[\"quantity\"]\n",
    "                    else:\n",
    "                        temp_dict[\"arguments\"][\"4\"]=i[\"quantity\"].text\n",
    "                    if(len(i[\"source\"])==0):\n",
    "                        temp_dict[\"arguments\"][\"5\"]=i[\"source\"]\n",
    "                    else:\n",
    "                        temp_dict[\"arguments\"][\"5\"]=i[\"source\"].text\n",
    "                    final_dict[\"extraction\"].append(temp_dict)\n",
    "            #PERSON\n",
    "            relations, work_list1 = extract_person_pos_relations(sentence)\n",
    "            final_list=[]\n",
    "            if(work_list1!=[]):\n",
    "                for i in work_list1:\n",
    "                    if i[1]!=[]:\n",
    "                        final_list.append(i)\n",
    "            if(final_list!=[]):\n",
    "                for i in final_list:\n",
    "                    temp_dict={}\n",
    "                    temp_dict[\"template\"]=\"WORK\"\n",
    "                    temp_dict[\"sentences\"]=[]\n",
    "                    temp_dict[\"sentences\"].append(sentence.text)\n",
    "                    temp_dict[\"arguments\"]={}\n",
    "                    temp_dict[\"arguments\"][\"1\"]=i[0].text\n",
    "                    stri=\"\"\n",
    "                    flag=True\n",
    "                    for j in i[1]:\n",
    "                        if(check_job(j.text)):\n",
    "                            stri=stri+j.text+\";\"\n",
    "                    if(stri!=\"\"):\n",
    "                        temp_dict[\"arguments\"][\"2\"]=stri[:-1]\n",
    "                    else:\n",
    "                        flag=False\n",
    "                    if(i[2]!=[]):\n",
    "                        stri=\"\"\n",
    "                        for j in i[2]:\n",
    "                            stri=stri+j.text+\";\"\n",
    "                        temp_dict[\"arguments\"][\"3\"]=stri[:-1]\n",
    "                    else:\n",
    "                        temp_dict[\"arguments\"][\"3\"]=\"\"\n",
    "                    if(len(i[3])!=0):\n",
    "                        stri=\"\"\n",
    "                        stri=stri+i[3].text+\";\"\n",
    "                        temp_dict[\"arguments\"][\"4\"]=stri[:-1]\n",
    "                    else:\n",
    "                        temp_dict[\"arguments\"][\"4\"]=\"\"\n",
    "\n",
    "                    if(flag):\n",
    "                        final_dict[\"extraction\"].append(temp_dict)\n",
    "        except:\n",
    "            print(\"hi\")\n",
    "            continue\n",
    "    test_sentences=read_single_file_table(filename)\n",
    "    for sentence_text in test_sentences:\n",
    "        try:\n",
    "            sentence = nlp(sentence_text)\n",
    "            sentence=merge_ents(sentence)\n",
    "            #PERSON\n",
    "            relations, work_list1 = extract_person_pos_relations_table(sentence)\n",
    "            final_list=[]\n",
    "            if(work_list1!=[]):\n",
    "                for i in work_list1:\n",
    "                    if i[1]!=[]:\n",
    "                        final_list.append(i)\n",
    "            if(final_list!=[]):\n",
    "                for i in final_list:\n",
    "                    temp_dict={}\n",
    "                    temp_dict[\"template\"]=\"WORK\"\n",
    "                    temp_dict[\"sentences\"]=[]\n",
    "                    temp_dict[\"sentences\"].append(sentence.text)\n",
    "                    temp_dict[\"arguments\"]={}\n",
    "                    temp_dict[\"arguments\"][\"1\"]=i[0].text\n",
    "                    stri=\"\"\n",
    "                    flag=True\n",
    "                    for j in i[1]:\n",
    "                        if(check_job(j.text)):\n",
    "                            stri=stri+j.text+\";\"\n",
    "                    if(stri!=\"\"):\n",
    "                        temp_dict[\"arguments\"][\"2\"]=stri[:-1]\n",
    "                    else:\n",
    "                        flag=False\n",
    "                    if(i[2]!=[]):\n",
    "                        stri=\"\"\n",
    "                        for j in i[2]:\n",
    "                            stri=stri+j.text+\";\"\n",
    "                        temp_dict[\"arguments\"][\"3\"]=stri[:-1]\n",
    "                    else:\n",
    "                        temp_dict[\"arguments\"][\"3\"]=\"\"\n",
    "                    if(len(i[3])!=0):\n",
    "                        stri=\"\"\n",
    "                        stri=stri+i[3].text+\";\"\n",
    "                        temp_dict[\"arguments\"][\"4\"]=stri[:-1]\n",
    "                    else:\n",
    "                        temp_dict[\"arguments\"][\"4\"]=\"\"\n",
    "                    \n",
    "                    if(flag):\n",
    "                        final_dict[\"extraction\"].append(temp_dict)\n",
    "        except:\n",
    "            print(\"hi\")\n",
    "            continue\n",
    "\n",
    "    json_filename=\"output \"+filename[:len(filename)-4] + \".json\"\n",
    "    json_object = json.loads(json.dumps(final_dict))\n",
    "    json_formatted_str = json.dumps(json_object, indent=4)\n",
    "\n",
    "    file = open(json_filename, \"w\")\n",
    "    n = file.write(json_formatted_str)\n",
    "    file.close()\n",
    "    print(\"JSON File for\",filename,\" printed to current working directory\")\n",
    "print(\"Extraction Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import errno\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import Tree\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import copy\n",
    "import os\n",
    "import string\n",
    "from itertools import chain\n",
    "from nltk.stem import PorterStemmer\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "def read_single_file(filename):\n",
    "    sent_tokens = []\n",
    "    f = open(filename,encoding=\"ascii\",errors=\"ignore\")\n",
    "    temp=f.read()\n",
    "    temp2=sent_tokenize(temp)\n",
    "    sent_tokens.extend(temp2)\n",
    "    f.close()\n",
    "    return sent_tokens\n",
    "\n",
    " \n",
    "def word_tokenization(sentences):\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #all_stops = stop_words | set(string.punctuation)\n",
    "    #word_tokens = [w for w in word_tokens if not w in all_stops]\n",
    "    word_tokens=[]\n",
    "    for i in sentences:\n",
    "        word_tokens.extend(word_tokenize(i))\n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "def word_lemmatization(words):\n",
    "    \n",
    "    lemmatize_word = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        lemmatize_word.append(lemmatizer.lemmatize(word))\n",
    "    return lemmatize_word\n",
    "\n",
    "def word_stemmatization(words):       \n",
    "    stemmatize_word = []\n",
    "    ps = PorterStemmer()\n",
    "    for word in words:\n",
    "        stemmatize_word.append(ps.stem(word))\n",
    "    return stemmatize_word\n",
    "\n",
    "def POS_tagging(words):\n",
    "    POS_tags = []\n",
    "    POS_tags = nltk.pos_tag(words)\n",
    "    return POS_tags\n",
    "\n",
    "\n",
    "def dependency_parsing(sentence):\n",
    "    dependency_parsed_tree =[]\n",
    "    en_nlp =spacy.load('en_core_web_sm')\n",
    "    doc = en_nlp(sentence)\n",
    "    sent= list(doc.sents)\n",
    "    for s in sent:\n",
    "        rootOfSentence = s.root.text\n",
    "    for token in doc:\n",
    "        dependency_parsed_tree.append([token.dep_,token.head.text,token.text])\n",
    "    return dependency_parsed_tree\n",
    "\n",
    "def display_dependency_parsing(sentence):\n",
    "    en_nlp =spacy.load('en_core_web_sm')\n",
    "    doc = en_nlp(sent_tokens[0])\n",
    "\n",
    "    def to_nltk_tree(node):\n",
    "        if node.n_lefts + node.n_rights > 0:\n",
    "            return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "        else:\n",
    "            return node.orth_\n",
    "    [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "    #displacy.render(doc, style='dep', jupyter=True)\n",
    "\n",
    "\n",
    "\n",
    "def wordnet_features(words):\n",
    "    synonymns_list = []\n",
    "    hypernyms_list = []\n",
    "    hyponyms_list = []\n",
    "    meronyms_list = []\n",
    "    holonyms_list = []\n",
    "    for word in words:\n",
    "        for i,j in enumerate(wn.synsets(word)):\n",
    "            synonymns_list.extend(wn.synset(j.name()).lemma_names())\n",
    "            hypernyms_list.extend(list(chain(*[l.lemma_names() for l in j.hypernyms()])))\n",
    "            hyponyms_list.extend(list(chain(*[l.lemma_names() for l in j.hyponyms()])))\n",
    "            meronyms_list.extend(list(chain(*[l.lemma_names() for l in j.part_meronyms()])))\n",
    "            holonyms_list.extend(list(chain(*[l.lemma_names() for l in j.part_holonyms()])))\n",
    "    return synonymns_list,hypernyms_list,hyponyms_list,meronyms_list,holonyms_list\n",
    "\n",
    "def named_entity_recognition(sentence):\n",
    "    entities=[]\n",
    "    entity_labels=[]\n",
    "    nlp = en_core_web_sm.load()\n",
    "    doc = nlp(sentence)\n",
    "    for X in doc.ents:\n",
    "        entities.append(X.text)\n",
    "        entity_labels.append(X.label_)\n",
    "    return entities,entity_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens :\n",
      "['Abraham Thomas Lincoln (February 12, 1809  April 15, 1865) was an American statesman, politician, and lawyer who served as the 16th president of the United States from 1861 until his assassination in April 1865.', 'Lincoln led the nation through the American Civil War, its bloodiest war and its greatest moral, constitutional, and political crisis.', 'He preserved the Union, abolished slavery, strengthened the federal government, and modernized the U.S. economy.', 'Born in Kentucky, Lincoln grew up on the frontier in a poor family.', 'Self-educated, he became a lawyer, Whig Party leader, state legislator and Congressman.', 'He left government to resume his law practice, but angered by the success of Democrats in opening the prairie lands to slavery, reentered politics in 1854.', 'He became a leader in the new Republican Party and gained national attention in 1858 for debating and losing to national Democratic leader Stephen A. Douglas in a Senate campaign.', 'He then ran for President in 1860, sweeping the North and winning.', 'Southern pro-slavery elements took his win as proof that the North was rejecting the Constitutional rights of Southern states to practice slavery.', 'They began the process of seceding from the union.']\n",
      "Word Tokens :\n",
      "['Abraham', 'Thomas', 'Lincoln', '(', 'February', '12', ',', '1809', 'April', '15']\n",
      "Stemma :\n",
      "['abraham', 'thoma', 'lincoln', '(', 'februari', '12', ',', '1809', 'april', '15'] ['Abraham', 'Thomas', 'Lincoln', '(', 'February', '12', ',', '1809', 'April', '15']\n",
      "Lemma :\n",
      "['Abraham', 'Thomas', 'Lincoln', '(', 'February', '12', ',', '1809', 'April', '15'] ['Abraham', 'Thomas', 'Lincoln', '(', 'February', '12', ',', '1809', 'April', '15']\n",
      "POS Tags :\n",
      "[('Abraham', 'NNP'), ('Thomas', 'NNP'), ('Lincoln', 'NNP'), ('(', '('), ('February', 'NNP'), ('12', 'CD'), (',', ','), ('1809', 'CD'), ('April', 'NNP'), ('15', 'CD')] ['Abraham', 'Thomas', 'Lincoln', '(', 'February', '12', ',', '1809', 'April', '15']\n",
      "D-Parse Tree :\n",
      "[['compound', 'Lincoln', 'Abraham'], ['compound', 'Lincoln', 'Thomas'], ['nsubj', 'was', 'Lincoln'], ['punct', 'Lincoln', '('], ['nmod', 'April', 'February'], ['nummod', 'February', '12'], ['punct', 'February', ','], ['npadvmod', 'February', '1809'], ['', '1809', ' '], ['appos', 'Lincoln', 'April'], ['nummod', 'April', '15'], ['punct', 'April', ','], ['nummod', 'April', '1865'], ['punct', 'Lincoln', ')'], ['ROOT', 'was', 'was'], ['det', 'statesman', 'an'], ['amod', 'statesman', 'American'], ['attr', 'was', 'statesman'], ['punct', 'statesman', ','], ['conj', 'statesman', 'politician'], ['punct', 'politician', ','], ['cc', 'politician', 'and'], ['conj', 'politician', 'lawyer'], ['nsubj', 'served', 'who'], ['relcl', 'statesman', 'served'], ['prep', 'served', 'as'], ['det', 'president', 'the'], ['amod', 'president', '16th'], ['pobj', 'as', 'president'], ['prep', 'president', 'of'], ['det', 'States', 'the'], ['compound', 'States', 'United'], ['pobj', 'of', 'States'], ['prep', 'served', 'from'], ['pobj', 'from', '1861'], ['prep', 'served', 'until'], ['poss', 'assassination', 'his'], ['pobj', 'until', 'assassination'], ['prep', 'assassination', 'in'], ['pobj', 'in', 'April'], ['nummod', 'April', '1865'], ['punct', 'was', '.']]\n",
      "Synonyms :\n",
      "['Abraham', 'Ibrahim', 'Thomas', 'Seth_Thomas', 'Thomas', 'Norman_Thomas', 'Norman_Mattoon_Thomas', 'Thomas', 'Lowell_Thomas', 'Lowell_Jackson_Thomas']\n",
      "Hypernyms :\n",
      "['domestic_sheep', 'Ovis_aries', 'Gregorian_calendar_month', 'large_integer', 'Gregorian_calendar_month', 'large_integer', 'typify', 'symbolize', 'symbolise', 'stand_for']\n",
      "Hyponyms :\n",
      "['boxcars', 'abound', 'accept', 'take', 'account', 'account_for', 'act', 'answer', 'appear', 'seem']\n",
      "Meronyms :\n",
      "['University_of_Nebraska', 'Candlemas', 'Candlemas_Day', 'Feb_2', 'Groundhog_Day', 'February_2', 'leap_day', 'bissextile_day', 'February_29', \"Lincoln's_Birthday\"]\n",
      "Holonyms :\n",
      "['Nebraska', 'Cornhusker_State', 'NE', 'Gregorian_calendar', 'New_Style_calendar', 'Gregorian_calendar', 'New_Style_calendar', 'United_States', 'United_States_of_America', 'America']\n",
      "NER :\n",
      "(['Abraham Thomas Lincoln', 'February 12, 1809  April 15, 1865', 'American', '16th', 'the United States', '1861', 'April 1865'], ['PERSON', 'DATE', 'NORP', 'ORDINAL', 'GPE', 'DATE', 'DATE'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sent_tokens=read_single_file(\"AbrahamLincoln.txt\")\n",
    "\n",
    "\n",
    "print(\"Sentence Tokens :\")\n",
    "print(sent_tokens[0:10])\n",
    "\n",
    "word_tokens=word_tokenization(sent_tokens)\n",
    "print(\"Word Tokens :\")\n",
    "print(word_tokens[0:10])\n",
    "\n",
    "stemma=word_stemmatization(word_tokens)\n",
    "print(\"Stemma :\")\n",
    "print(stemma[0:10],word_tokens[0:10])\n",
    "\n",
    "lema=word_lemmatization(word_tokens)\n",
    "print(\"Lemma :\")\n",
    "print(lema[0:10],word_tokens[0:10])\n",
    "\n",
    "pos=POS_tagging(word_tokens)\n",
    "print(\"POS Tags :\")\n",
    "print(pos[0:10],word_tokens[0:10])\n",
    "print(\"D-Parse Tree :\")\n",
    "print(dependency_parsing(sent_tokens[0]))\n",
    "\n",
    "syn,hyper,hypo,mero,holo=wordnet_features(word_tokens)\n",
    "print(\"Synonyms :\")\n",
    "print(syn[0:10])\n",
    "print(\"Hypernyms :\")\n",
    "print(hyper[0:10])\n",
    "print(\"Hyponyms :\")\n",
    "print(hypo[0:10])\n",
    "print(\"Meronyms :\")\n",
    "print(mero[0:10])\n",
    "print(\"Holonyms :\")\n",
    "print(holo[0:10])\n",
    "print(\"NER :\")\n",
    "print(named_entity_recognition(sent_tokens[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
